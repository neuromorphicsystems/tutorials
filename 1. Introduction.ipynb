{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: An introduction to event-based processing\n",
    "\n",
    "Prepared by the International Centre for Neuromorphic Systems (ICNS) at Western Sydney University, adapted from the short course presented at the AMOS Conference 2023.\n",
    "\n",
    "## Introduction\n",
    "***What you will learn***\n",
    "In this tutorial, we will work with real-world event-based data to learn how to load and process it. \n",
    "\n",
    "By the end of this tutorial, you will...\n",
    "* Be able to load and read events from an event-based data file.\n",
    "* Understand the data representation from an event-based camera and how to interact with it.\n",
    "* Extract important information for the event-based data stream.\n",
    "* Be able to write an iterative event-based algorithm for processing event-based data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame-based Image Data\n",
    "\n",
    "Let's start by examining the differences between frame-based data and neuromorphic event-based data. Event-based cameras convert the visual word to data in a very different way to conventional integrating cameras, and therefore the way in which we need to work the data and the way we need to visualise the data is inherently different. Integrating cameras work by measuring the amount of light that has fallen on each pixel over a predetermined amount of time (the exposure time). This data is then read out by circuitry in the camera to produce a frame.\n",
    "\n",
    "Frame-based images are therefore nothing more than matrices of numbers that represent the light intensity at each pixel. Putting aside the issues of image compression and image storage formats, the raw data contained inside each frame is simply a measure of the amount of light that the camera detected at that pixel. That is also how the data is stored on a computer. For simplicity sake, let's assume that we have a grayscale camera that just measures the total amount of light that falls on each pixel, in that case the data produced would be a 2D matrix containing numbers that represents how bright or dark that pixel should appear on the screen. That 2D matrix is what we tend to call a frame, and the dimensions of the matrix are the row and height of the image. \n",
    "\n",
    "Let's try a very simple example of how we can create simple grayscale images using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "\n",
    "\n",
    "height = 100\n",
    "width = 100\n",
    "\n",
    "# Create a 2D matrix with a size of (100, 100) pixels\n",
    "small_image = numpy.zeros((height, width))\n",
    "\n",
    "small_image[20:40,20:40] = 255\n",
    "small_image[20:40,60:80] = 196\n",
    "small_image[60:80,20:40] = 128\n",
    "small_image[60:80,60:80] = 64\n",
    "\n",
    "matplotlib.pyplot.imshow(small_image, cmap=\"gray\")\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terribly exciting, but it is important to understand exactly how we can represent and work with images in Python. We will be using these image frames to \"see\" what's happening in the output of an event-based camera.\n",
    "\n",
    "Note: In reality though, we often want to store colour images, rather than grayscale ones. To do this, we instead store three different values for each pixel, denoting the amount of red, green, and blue light per pixel. This is often called RGB colour, and simply requires us to store a 3D matrix for each image. Unfortunately, this method of working with colour does not carry across to  event-based cameras, mostly as colour is an additive property and requires absolute measures of intensity. Simply detecting changes in intensity doesn't yield the same information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event-based Image Data\n",
    "\n",
    "Let's take a look at how event-based cameras produce data. The camera doesn't produce 2D matrices of number (i.e. a frame), but rather produces a stream of change events. These are stored in a format known as Address-Event Representation (AER). This format is widely used in neuromorphic systems to pass data from one system to another. Whereas the basic unit of data produced by a conventional camera is a frame, the basic unit used by an event-based camera is an event. These events indicate that a specific change in light intensity happened at a specific pixel in the sensor at a specific time. \n",
    "\n",
    "To get started, we'll need to ensure that the correct Python libraries are installed. We will need a library called `event_stream`. You can install it (both on your local machine and on Google Colaboratory) using the following commmand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required dependencies (if not already installed). This is required to install the required packages in Google Colaboratory.\n",
    "!pip3 install event_stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some data to use in this tutorial. We can download some sample data from the Github repository using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "\n",
    "# Download the sample data from Github\n",
    "# two_horses was recorded with a DAVIS 346, whilst strange_horses was recorded with a Prophesee Gen4\n",
    "base_url = \"https://github.com/neuromorphicsystems/tutorials/raw/main/data\"\n",
    "two_horses_path, _ = urllib.request.urlretrieve(f\"{base_url}/two_horses.es\", \"two_horses.es\")\n",
    "strange_horses_path, _ = urllib.request.urlretrieve(f\"{base_url}/strange_horses.es\", \"strange_horses.es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading event-based data using event_stream\n",
    "\n",
    "We will start by loading some event-based data using the ```Event Stream``` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import event_stream\n",
    "\n",
    "\n",
    "# Load some data from the DAVIS 346 sensor\n",
    "decoder = event_stream.Decoder(two_horses_path)\n",
    "\n",
    "print(f\"Reading data from the DAVIS 346 sensor\")\n",
    "print(f\"Decoder type: {decoder.type}\")\n",
    "print(f\"Sensor resolution: {decoder.width} x {decoder.height}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import event_stream\n",
    "\n",
    "\n",
    "# Load some data from the Prophesee Gen4 sensor\n",
    "decoder = event_stream.Decoder(strange_horses_path)\n",
    "\n",
    "print(f\"Reading data from the Prophesee Gen4 sensor\")\n",
    "print(f\"Decoder type: {decoder.type}\")\n",
    "print(f\"Sensor resolution: {decoder.width} x {decoder.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the code above can be used to read data from both the DAVIS346 and the Prophesee Gen4 sensors as the output format of both sensors use the same data format. We can also access the width and height properties provided by the decoder to get the size of the sensor. The rest of this tutorial works for both data files, and we encourage you to experiment and try the different data files.\n",
    "\n",
    "\n",
    "In summary, the code above loads the data from an event-based file and prints out some important information about the data contained within the file. \n",
    "\n",
    "**If these examples run, you have successfully configured your Python environment, either on your local computer or on Google Colaboratory.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the tutorial\n",
    "\n",
    "The lines below determine which file will be used throughout this tutorial. By setting the variable to the specific file, you can switch between the two data files provided in this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set `file_path` the file path to be used throughout this tutorial\n",
    "# two_horses was recorded with a DAVIS 346, whilst strange_horses was recorded with a Prophesee Gen4\n",
    "\n",
    "file_path = two_horses_path\n",
    "# file_path = strange_horses_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Understanding event-based processing: Encoders and Decoders\n",
    "\n",
    "The ```event_stream``` library uses the concepts of encoders and decoders to work with event-based files. \n",
    "\n",
    "**Decoders** interpret files and produce a stream of events for processing, whilst encoders write new event-based data to a data file. \n",
    "\n",
    "Event-based data is asynchronously generated, meaning that there is not fixed frame-rate or event-rate. The activity in the visual scene determines the number of events generated during any period. This complicates the process of saving and recalling data as the data does not appear in convenient and organised structures such as frames. In fact, we tend to store event-based in a way that allows it to be read out iteratively, event-by-event. In principle, this is how the standard decoder in ```event-stream``` operates. \n",
    "\n",
    "The ```decoder.type``` property shows the type of the event-based file. Whilst the underlying concepts around event-based cameras are broadly similar for all the sensors available, there are sensors that implement additional functionality which also produces data in an event-based fashion.\n",
    "\n",
    "The most common decoder type (and the one used throughout these tutorials) is the ```dvs``` data type.\n",
    "\n",
    "In practice, reading the data event-by-event from the file is inefficient as it incurs a significant overhead on each event. For performance reasons, we tend to buffer the data into groups of events. The ```event_stream``` decoder implements this by reading in chunks of events and then allowing you to iterate through them. \n",
    "\n",
    "We can explore this behaviour by interacting with the decoder as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import event_stream\n",
    "\n",
    "\n",
    "decoder = event_stream.Decoder(file_path)\n",
    "\n",
    "# Read the first chunk of events from the file.\n",
    "# The decoder is an iterator, and we can retrieve the first chunk of events by using the next function.\n",
    "chunk = next(decoder)\n",
    "print(f\"The first chunk contains {len(chunk)} events.\")\n",
    "\n",
    "# Manually print out the first three events in this chunk\n",
    "print(\n",
    "    f\"x: {chunk[0]['x']}, y: {chunk[0]['y']}, t: {chunk[0]['t']}, p: {chunk[0]['on']}\"\n",
    ")\n",
    "print(\n",
    "    f\"x: {chunk[1]['x']}, y: {chunk[1]['y']}, t: {chunk[1]['t']}, p: {chunk[1]['on']}\"\n",
    ")\n",
    "print(\n",
    "    f\"x: {chunk[2]['x']}, y: {chunk[2]['y']}, t: {chunk[2]['t']}, p: {chunk[1]['on']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Address-Event Representation (AER)\n",
    "\n",
    "Event-based cameras output data in a format known as Address-Event Representation (AER). This format is widely used in neuromorphic systems to pass data from one system to another. The unit of data exchange in an AER system is an event, which indicates a message from a specific source, which is a specific pixel in the case of an event-based sensor.\n",
    "\n",
    "Each AER event from the camera is represented by four values:\n",
    "\n",
    "$e_i = [x, y, t, p]$, \n",
    "\n",
    "in which:\n",
    "* $(x,y)$ are the coordinates of the pixel that generated the event\n",
    "* $t$ is the relative timestamp at which the event occurred (in microseconds)\n",
    "* $p$ is the polarity of the event (*True* for an **ON** event, *False* for an **OFF** event)\n",
    "\n",
    "The polarity of an event indicates whether the contrast at a given pixel $(x,y)$ increased (indicated by an **ON** event) or decreased (indicated by an **OFF** event) at the time given by timestamp $t$. The timestamp for each event indicates the relative time at which the event was generated by the sensor. This is a relative timestamp with microsecond resolution. It can directly be used to measure the time between events, but is referenced to an arbitrary point in time (often, but not always, the time at which the camera was first powered on).\n",
    "\n",
    "**Note**: The timestamps in the provided recordings start at 0. This is not always the case. Always check the the timestamp of the first event in a recording to ensure that you have the correct time offset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with timestamps\n",
    "\n",
    "Since the timestamps always increase, we can use the timestamps to easily calculate the length of the recording:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import event_stream\n",
    "import numpy\n",
    "\n",
    "\n",
    "# Read all the events into one large N x 4 matrix, where N is the number of events\n",
    "decoder = event_stream.Decoder(file_path)\n",
    "events = numpy.concatenate([chunk for chunk in decoder])\n",
    "\n",
    "print(f\"Total events: {len(events)}\")\n",
    "print(f\"First timstamp: {events[0]['t']}\")\n",
    "print(f\"Last timestamp: {events[-1]['t']}\")\n",
    "\n",
    "duration = events[-1][\"t\"] - events[0][\"t\"]\n",
    "print(f\"Recording length: {duration / 1e6} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamps from an event-based recording contain a lot of useful information on their own. As event-based cameras produce events in response to visual changes in the scene, the rate at which events are generated acts as an indicator for the activity in the scene. If we plot the timestamps as a vector (i.e. against their index), we get a plot in which the gradient of the line indicates the activity in the scene. \n",
    "\n",
    "We can use the code below to generate a simple plot of the data in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import event_stream\n",
    "import matplotlib.pyplot\n",
    "\n",
    "decoder = event_stream.Decoder(file_path)\n",
    "\n",
    "timestamps = []\n",
    "for chunk in decoder:\n",
    "    for t, x, y, p in chunk:\n",
    "        timestamps.append(t)\n",
    "\n",
    "matplotlib.pyplot.plot(timestamps)\n",
    "matplotlib.pyplot.xlabel(\"Event Index\")\n",
    "matplotlib.pyplot.ylabel(\"Time (µs)\")\n",
    "matplotlib.pyplot.title(\"Plot of Timestamps vs. Index\")\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing algorithms for event-based processing\n",
    "\n",
    "You may have noticed that this code is not terribly efficient. For instance, we could have plotted the curve above by loading all the events into a single matrix as we did in the prior example.  Instead, the code iterates over the events and processes events individually. This method, whilst not efficient for this data, is important in understanding the event-based nature of the data and how to process it in an event-based way. This becomes especially critical when designing and building event-based systems to run in real-time and for real-world applications.\n",
    "\n",
    "The iterative approach above processes each event individually and in the order in which they are generated. Loading in all the data at once works for pre-recorded and offline files, but can quickly cause problems with large files or when the event-rate from the camera suddenly increases. This method also does not easily extend to real-time systems, when data needs to be processed as it is generated by the sensor. We can batch the events, but then we need to decide if we want to group them temporally (i.e. over a set period of time) or by volume (i.e. wait for a fixed number of events to occur). Both of these approaches can introduce latency and complexity into a processing system.\n",
    "\n",
    "Iterative approaches solve many of these problems, especially when the actions performed on each event are quick to execute. They will wait if there are no events to process and can buffer events if they occur faster than they can be processed. This provides the added benefit of activity-driven processing, where data processing is only performed when events are generated. This style of processing is well suited to high-speed hardware implementations, such such as a Field Gate Programmable Array (FPGA) or dedicated neuromorphic hardware accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Global Activity: An iterative algorithm for better event-rate visualisation \n",
    "\n",
    "Whilst this plot of the timestamps is interesting, it only shows the speed at which the event rates are changing. This does not give us a sense of what is happening in the data.\n",
    "\n",
    "As it turns out, calculating a meaningful rate from non-uniformly sampled data is a hard problem. One possibility is to use time bins, but these bins cannot be shorter than the integration window. For an integration window of 200 ms, you can only generate 5 event rate samples per second, which is a very coarse approximation.\n",
    "\n",
    "Another option is a sliding window. The approximation is much better but it requires a lot of memory (usually a First-In-First-Out (FIFO) buffer). Extending the implementation to event-based data is not a straightforward task.\n",
    "\n",
    "To overcome this, we need to use something that calculates a value similar in meaning to the event rate, but one that can be efficiently implemented and calculated for event-based data. We call this the Global Activity Rate. To do this, we implement a simple leaky integrator in which we do the following:\n",
    "\n",
    "* Exponentially decay the previous activity over the inter-event time interval (***delta_t***). This is the **leak**.\n",
    "* Increment the activity by 1 for each event. This is the **integration**.\n",
    "\n",
    "We implement the leak as follows:\n",
    "\n",
    "$ activity = e^{((-\\Delta t) / \\tau)}$\n",
    "\n",
    "Where $\\Delta t$ is the increase in time since the last event, and $\\tau$ is a fixed time interval over which we want to decay the value. We can pick $\\tau$ to change the time intervals of interest. It is worth exploring the effect of changing $tau$ on the graph produced.\n",
    "\n",
    "The beauty of this algorithm is that it uses only two memory variables (both ***floats***). This makes the algorithm much faster and memory efficient than other event-rate algorithm.\n",
    "\n",
    "**Note:** Some of you might notice that these equations look very similar to those for a leaky-integrate-and-fire neuron model. This is because it shares the same underlying structure! If you add a threshold to trigger an action when the global activity increases beyond a certain value (with a refractory period for good measure), you end up with a full-fledged leaky integrate-and-fire neuron.\n",
    "\n",
    "We can calculate the Global Activity using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import event_stream\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "decoder = event_stream.Decoder(file_path)\n",
    "\n",
    "event_activity = 0.0\n",
    "previous_t = 0  # µs\n",
    "tau = 100.0     # µs\n",
    "\n",
    "ts = []\n",
    "activity = []\n",
    "for chunk in decoder:\n",
    "    for t, x, y, p in chunk:\n",
    "        delta_t = t - previous_t\n",
    "        event_activity *= math.exp(-float(delta_t) / tau)  # Leak\n",
    "        event_activity += 1                                # Integrate\n",
    "        previous_t = t\n",
    "        ts.append(t)\n",
    "        activity.append(event_activity)\n",
    "\n",
    "plt.plot(ts, activity)\n",
    "plt.xlabel(\"Timestamps (microseconds)\")\n",
    "plt.ylabel(\"Activity\")\n",
    "plt.title(\"Plot of Global Activity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on visualising event-based data\n",
    "\n",
    "One of the challenges with event-based data is that the raw recordings are not in a format that is readily accessible for viewing. Unlike the output of a conventional camera, there are no straightforward ways to replay a recording. There are no mature tools for viewing, or even working, with event-based camera data. Instead, the data is often imported into a programming environment and then converted to a visual representation using custom-written scripts. The field also has not yet created any standard methods for storing, processing, or presenting data.\n",
    "\n",
    "Frames present the most immediate and compelling method to visualise event-based data given that their format matches the output format of most monitors and displays (which are themselves frame-based). However, creating a 2D representation of event-based data requires a post-processing step that inherently discards or aggregates data and the choice of post-processing algorithm can have a significant impact on the interpretation of the data. \n",
    "\n",
    "**Note:** The process of generating frames from event-based data can hide important aspects of the data itself, or lend itself to incorrect interpretation of the true motion in the scene. Generating frames produces a single view on the data. Other views may exist and may be more suitable to the underlying data. This may also vary from application to application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating frames from event-based data\n",
    "In this tutorial, we will explore a simple method for converting event-based data into frames. This process will render conventional frames from the event-stream and compile those into an animation or a video, allowing us to interact with the data in a familiar format. These techniques are often used to explore new event-based data, to perform quick checks on the validity of data, and to diagnose and debug algorithms. Keep in mind that this is not a complete representative of the underlying data stream.\n",
    "\n",
    "Most methods for converting event-based data into frames makes use of the following set of steps:\n",
    "\n",
    "1. Identify a set time interval over which to generate frames\n",
    "2. Place events into time bins determined by the time intervals\n",
    "3. Accumulate the events in each time bin into a 2D frame representation \n",
    "4. Collate the frame from each time interval into a video\n",
    "\n",
    "A multitude of ways exist to perform the third step in the above process, and the choice of algorithm is highly dependent on the nature of the data to be visualise. The conversion of the list of spatio-temporal events to a 2D frame is inherently lossy. \n",
    "\n",
    "For our simple method, we will keep track of all the pixels that have events occurring during each time bin. We will then mark them as either containing an ON event or an OFF event based on the last event received for that pixel in each time interval. This will result in a frame for each time interval showing the pixels at which events occurred.\n",
    "\n",
    "The following code renders simple frames from the event-stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import event_stream\n",
    "import matplotlib.pyplot\n",
    "import matplotlib.animation\n",
    "import numpy\n",
    "import IPython.display\n",
    "\n",
    "decoder = event_stream.Decoder(file_path)\n",
    "events = numpy.concatenate([chunk for chunk in decoder])\n",
    "\n",
    "frame_duration = 1000000  # µs\n",
    "\n",
    "# Generate the frames from the event stream\n",
    "next_frame_t = 0\n",
    "frame = numpy.zeros((decoder.width, decoder.height))\n",
    "frames = []\n",
    "for t, x, y, p in events:\n",
    "    if t >= next_frame_t:\n",
    "        frames.append(frame)\n",
    "        frame = numpy.zeros((decoder.width, decoder.height))\n",
    "        next_frame_t += frame_duration\n",
    "    frame[x, y] = 1 if p else -1\n",
    "\n",
    "# Display the frames inline using Matplotlib Animations\n",
    "# See https://matplotlib.org/stable/users/explain/colors/colormaps.html to choose a colormap\n",
    "# With coolwarm, ON events are red and OFF events are blue\n",
    "figure, subplot = matplotlib.pyplot.subplots()\n",
    "canvas = subplot.imshow(frame.T, origin=\"lower\", cmap=\"coolwarm\")\n",
    "\n",
    "\n",
    "def animate(index):\n",
    "    canvas.set_data(frames[index].T)\n",
    "    return []\n",
    "\n",
    "\n",
    "animation = matplotlib.animation.FuncAnimation(figure, animate, frames=len(frames))\n",
    "IPython.display.HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises to try:\n",
    "\n",
    "* Try changing the ***frame_duration*** to see the effect of increasing the frame rate on the data. \n",
    "* Try implementing an alternative frame conversion method that counts the number of events at each pixel. \n",
    "* Try blending the current frame with a weighted version of the previous frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise and filtering\n",
    "\n",
    "You may have noticed that the recordings on the camera contain noise. From the event-rate curves, it is clear that the cameras generate events even when there is no explicit activity in the scene. This noise is also visible in the frames as individual events that seemingly occur randomly. Whilst there are different types of noise events,  we will examine two of them in these tutorials:\n",
    "\n",
    "* **Spurious events** are random events from pixels that do not necessarily correspond to a contrast change.\n",
    "* **Hot pixels** are noisy pixels that generate lots of events\n",
    "\n",
    "These are caused by a variety of factors, including photodiode noise, transistor mismatch in the circuitry, internal noise in the sensor, temperature effects. The nature and cause of these noise events are an active subject of research in the neuromorphic and event-based sensor community. \n",
    "\n",
    "There are two main approaches for dealing with noise:\n",
    "\n",
    "* Design algorithms that are tolerant of noise\n",
    "* Filter out the noise prior to processing\n",
    "\n",
    "In this tutorial, we will explore algorithms for pre-processing algorithms for filtering and removing these noise events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbour support noise filtering\n",
    "\n",
    "A popular way to remove spurious background noise events is to use a temporal neighbour support filtering algorithm. This method works by eliminating isolated events that do not occur near other event, both spatially and temporally. The algorithm is iterative and is often implemented in hardware directly. \n",
    "\n",
    "The algorithm works by keeping track of the last time each pixel in the camera generated an event. When a new event occurs, the neighbouring pixels are checked to see if an event occurred within an specified time window. If none of the neighbouring events have generated an event recently, the event is labelled as a noise event and discarded. If at least one of the neighbouring pixels has generated an event recently, then the pixel is said to be supported and is retained. \n",
    "\n",
    "This filtering process is illustrated below:\n",
    "\n",
    "![](figures/noise_filtering_illustration.png)\n",
    "\n",
    "This filter is very easy to implement and works well with many scenes but it does have some drawbacks. \n",
    "\n",
    "Firstly, it is worth noting that this filter may incorrectly discard the first event from a valid motion across the sensor surface. In most scenes, this is not noticeable as there are usually enough subsequent events to compensate for it. However, when tracking objects such as faint stars across the sensor surface, these lost events may be significant and this filter may not be the appropriate choice. \n",
    "\n",
    "The time window selected for the filter is particularly important and can greatly affect the efficacy of the filtering process. A longer time window will generally keep more events.\n",
    "\n",
    "The neighbour support noise filter can be implemented as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import event_stream\n",
    "import numpy\n",
    "\n",
    "time_window = 10000       # µs, a shorter time window filters more events\n",
    "frame_duration = 1000000  # µs (used for plotting the events as a video)\n",
    "\n",
    "# Read in all the events from the file\n",
    "decoder = event_stream.Decoder(file_path)\n",
    "events = numpy.concatenate([chunk for chunk in decoder])\n",
    "\n",
    "# Filter the events by examining the neighbouring four pixels to see if an event\n",
    "# has occurred within the specified time_window. If so, keep that event.\n",
    "timestamps = numpy.zeros((decoder.width, decoder.height))\n",
    "filtered_events = []\n",
    "for t, x, y, on in events:\n",
    "    timestamps[x][y] = t\n",
    "    if (\n",
    "        (x > 0 and timestamps[x - 1][y] + time_window > t)\n",
    "        or (x < decoder.width - 1 and timestamps[x + 1][y] + time_window > t)\n",
    "        or (y > 0 and timestamps[x][y - 1] + time_window > t)\n",
    "        or (y < decoder.height - 1 and timestamps[x][y + 1] + time_window > t)\n",
    "    ):\n",
    "        filtered_events.append((t, x, y, on))\n",
    "\n",
    "# Print a summary of the filtering results\n",
    "print(f\"Original recording contained {len(events)} events.\")\n",
    "print(f\"Filtered recording contains {len(filtered_events)} events.\")\n",
    "print(f\"Reduced by {((len(events) - len(filtered_events)) / len(events)) * 100:.2f} %\")\n",
    "\n",
    "# Generate the frames from the event stream\n",
    "next_frame_t = 0\n",
    "frame = numpy.zeros((decoder.width, decoder.height))\n",
    "frames = []\n",
    "for t, x, y, p in filtered_events:\n",
    "    if t >= next_frame_t:\n",
    "        frames.append(frame)\n",
    "        frame = numpy.zeros((decoder.width, decoder.height))\n",
    "        next_frame_t += frame_duration\n",
    "    frame[x, y] = 1 if p else -1\n",
    "\n",
    "# Display the frames inline using Matplotlib Animations\n",
    "# See https://matplotlib.org/stable/users/explain/colors/colormaps.html to choose a colormap\n",
    "# With coolwarm, ON events are red and OFF events are blue\n",
    "figure, subplot = matplotlib.pyplot.subplots()\n",
    "canvas = subplot.imshow(frame.T, origin=\"lower\", cmap=\"coolwarm\")\n",
    "\n",
    "\n",
    "def animate(index):\n",
    "    canvas.set_data(frames[index].T)\n",
    "    return []\n",
    "\n",
    "\n",
    "animation = matplotlib.animation.FuncAnimation(figure, animate, frames=len(frames))\n",
    "IPython.display.HTML(animation.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises to try:\n",
    "* Implement the full search of the 3x3 and 4x4 area around the event. Numpy array slicing can perform this operation very efficiently.\n",
    "* Explore requiring more than one pixel of support when filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An event-based filtering pipeline\n",
    "\n",
    "Finally, the above filter can be used to build an entirely event-based building block that ingests events and emits events. In the code below, the `event_stream` library is used to both read the events and to write the filtered events out to a new file. Once this is done, other algorithms can then operate on the filtered events without any knowledge of the upstream processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import event_stream\n",
    "import numpy\n",
    "\n",
    "time_window = 1000  # µs, a shorter time window filters more events\n",
    "\n",
    "decoder = event_stream.Decoder(file_path)\n",
    "encoder = event_stream.Encoder(\n",
    "    f\"{file_path[:-3]}_filtered.es\", decoder.type, decoder.width, decoder.height\n",
    ")\n",
    "\n",
    "timestamps = numpy.zeros((decoder.width, decoder.height))\n",
    "\n",
    "for chunk in decoder:\n",
    "    new_chunk = []\n",
    "    for t, x, y, on in chunk:\n",
    "        timestamps[x][y] = t\n",
    "        if (\n",
    "            (x > 0 and timestamps[x - 1][y] + time_window > t)\n",
    "            or (x < decoder.width - 1 and timestamps[x + 1][y] + time_window > t)\n",
    "            or (y > 0 and timestamps[x][y - 1] + time_window > t)\n",
    "            or (y < decoder.height - 1 and timestamps[x][y + 1] + time_window > t)\n",
    "        ):\n",
    "            new_chunk.append((t, x, y, on))\n",
    "    encoder.write(numpy.array(new_chunk, dtype=event_stream.dvs_dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='installation'></a>\n",
    "## Installing on your own computer\n",
    "\n",
    "These tutorials can be run directly on Google Colaboratory and do not require you to install them locally. However, if you would like to run these tutorials on your own computer, you will need to download the repository from Github and ensure that a few Python packages are installed on your system. You will need to install ```event_stream```, a library which provides an easy method for interacting with [Event Stream](https://github.com/neuromorphic-paris/event_stream) files (formatted .es) containing data from an event-based camera.\n",
    "\n",
    "NOTE: Event Stream is not the only file format for event-based data.  There are tools to assist in converting from other formats to ```Event Stream```, including the open-source [loris](https://github.com/neuromorphic-paris/loris) tool. However, we will exclusively use the .es file format here.\n",
    "\n",
    "Libraries and sample code to use `Event Stream` files are available for Matlab, Python, and C++ at the [official repository](https://github.com/neuromorphicsystems/event_stream). \n",
    "\n",
    "For this tutorial, we will be using the Python interface.  `Event Stream` needs `numpy` to run and we will use `matplotlib` to display results. If you do not have these installed, you will need to do use the following commands to install them:\n",
    "\n",
    "```sh\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "**Note:** If you are using `Anaconda`, don't forget to use `conda` instead of pip!\n",
    "\n",
    "You may get errors during the installation of the ```event_stream``` library if the installed version of ```numpy``` is not compatible. If this occurs, the easiest solution is to use a tool like ```virtualenv``` or ```conda``` to create a new virtual environment for Python.\n",
    "\n",
    "Once these two tools are installed, we can begin to explore event-based data using some example data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07c4f84319129d1c976bad471c8642b46ea9b7b96fed5777f2c683a8a5a74ac2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
